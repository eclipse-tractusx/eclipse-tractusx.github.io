---
sidebar_position: 1
title: Layers & Modules
---

This chapter gives an overview how the Agent standard and Kit should be implemented in terms of layers and modules.

For more information see

* Our [Adoption](../adoption-view/intro) guideline
* The [High-Level Architecture](architecture)
* Our [Reference Implementation](reference)
* The [Deployment](../operation-view/deployment) guide

In this context generic building blocks were defined (see Figure 4) which can be implemented with different open source or COTS solutions. In the scope of Catena-X project these building blocks are instantiated with a reference implementation based on open source components (the Knowledge Agents KIT). The detailed architecture following this reference implementation can be found here: <https://catenax-ng.github.io/product-knowledge/docs/architecture>.

[![Architecture High-Level](/img/knowledge-agents/layer_architecture_small.png)](/img/knowledge-agents/layer_architecture.png)

In the following paragraphs, all building blocks relevant for this standard are introduced:

## Semantic Models

Ontologies, as defined by W3C Web Ontology Language OWL 2 (<https://www.w3.org/OWL/>) standard, provide the core of the KA catalogue. By offering rich semantic modelling possibilities, they contribute to a common understanding of existing concepts and their relations across the data space participants. To increase practical applicability, this standard contains an overview about most important concepts and best practices for ontology modelling relevant for the KA approach (see chapter 5). OWL comes with several interpretation profiles (<https://www.w3.org/TR/owl2-profiles/>) for different types of applications. For model checking and data validation (not part of this standard), we propose the Rule Logic (RL) profile. For query answering/data processing (part of this standard), we apply the Existential Logic (EL) profile (on the Dataspace Layer) and the Query Logic (QL) profile (on the Binding Layer).

### Ontology Editing & Visualization

To create and visualize ontology models, dedicated tooling is advised. For this purpose, various open source tools (e.g. Protegé) or commercial products (e.g. metaphacts) are available. We hence standardize on the ubiquitous RDF Terse Triple Language TTL (<https://www.w3.org/TR/turtle/>) format which is furthermore able to divide/merge large ontologies into/from modular domain ontology files.

### Ontology Management

To achieve model governance, a dedicated solution for ontology management is necessary. Key function is to give an overview about available models and their respective meta data and life cycle (e.g. in work, released, deprecated). Because of the big parallels, it is today best practice to perform ontology management through modern and collaborative source code versioning systems. The de-facto standard in this regard is GIT (in particular: its http/s protocol variant, including an anonymous read-only raw file access to release branches). In the following, we call the merged domain ontology files in a release branch “the” (shared) Semantic Model (of that release). For practicability purposes, the Data Consumption and the Binding Layer could be equipped with only use-case and role-specific excerpts of that Semantic Model. While this may affect the results of model checking and validity profiles, it will not affect the query/data processing results.

## Data Consumption Layer/Query Definition

This layer comprises all applications which utilize provided data and functions of business partners to achieve a direct business impact and frameworks which simplify the development of these applications. Thus, this layer focuses on using a released Semantic Model (or a use-case/role-specific excerpt thereof) as a vocabulary to build flexible queries (Skills) and integrating these Skills in data consuming apps.

We rely on SPARQL 1.1 specification (<https://www.w3.org/TR/sparql11-query/>) as a language and protocol to search for and process data across different business partners. As a part of this specification, we support the QUERY RESULTS JSON (<https://www.w3.org/TR/sparql11-results-json/>) and the QUERY RESULTS XML (<https://www.w3.org/TR/rdf-sparql-XMLres/>) formats to represent both the answer sets generated by SPARQL skills and the sets of input parameters that a SPARQL skill should be applied to. For answer sets, additional formats such as the QUERY RESULTS CSV and TSV (<https://www.w3.org/TR/sparql11-results-csv-tsv/>) format may be supported. Required is the ability to store and invoke SPARQL queries as parameterized procedures in the dataspace; this is a KA-specific extension to the SPARQL endpoint and is captured a concise Openapi specification in the following (<https://app.swaggerhub.com/apis/Catena-X/CX-KA-Agent-Specification/0.8.7#/>). Also part of that specification is an extended response behaviour which introduces the warning status code “203” and a response header “cx_warning” bound to a JSON structure that lists abnormal events or trace information that appeared during the processing.

### Skill Framework

Consumer/Client-side component, which is connected to the consumer dataspace components (the Matchmaking Agent via SPARQL, optionally: the EDC via the Data Management API). It is at least multi-user capable (can switch/lookup identities of the logged-in user), if not multi-tenant capable (can switch Matchmaking Agents and hence ED Connectors). It looks up references to Skills in the Dataspace and delegates their execution to the Matchmaking Agent. The Skill framework may maintain a “conversational state” per user (contextual memory which is a kind of graph/data set) which drives the workflow. It may also help to define, validate and maintain Skills in the underlying Dataspace Layer.

### Query/Skill Editor

To systematically build and maintain Skills, a query editor for easy construction and debugging queries is advisable. The skill editor should support syntax highlighting for the query language itself and it may support auto-complete based on the Semantic Model. A skill editor could also have a graphical model in which a procedure can be composed out of pre-defined blocks. Finally, a skill editor should have the ability to test-drive the execution of a skill (maybe without storing/publishing the skill and making any “serious” contract agreements in the dataspace and based on sample data).

### Data Consuming App

Application that utilizes data of data providers to deliver added value to the user (e.g. CO2 footprint calculation tool). Skills can be easily integrated in these apps as stored procedure. Hence, skill and app development can be decoupled to increase efficiency of the app development process. For more flexible needs, Skills could be generated ad-hoc from templates based on the business logic and app data. The Data Consuming App could integrate a Skill Framework to encapsulate the interaction with the Dataspace Layer. The Consuming App could also integrate a Query/Skill Editor for expert users.

## Dataspace Layer

The base Dataspace-building technology is the Eclipse Dataspace Connector (EDC) which should be extended to operate as a HTTP/S contracting & transfer facility for the SPARQL-speaking Matchmaking Agent. To resolve dataspace offers and addresses using the ontological vocabulary, the Matchmaking Agent keeps a default meta-graph, the Federated Catalogue, that is used to host the Semantic Model and that is regularly synchronized with the relevant dataspace information including the offers of surrounding business partners/EDCs.

[![Dataspace Layer](/img/knowledge-agents/dataspace_layer_small.png)](/img/knowledge-agents/dataspace_layer.png)

### EDC*

Actually, the Eclipse Dataspace Connector (see Catena-X Standard CX-00001) consists of two components which both have to be extended (using the EDC extension mechanism) for KA enablement. The Control Plane hosts the actual management/negotiation engine and is usually a singleton that is exposing

an internal (api-key secured) API for managing the control plane by administrative accounts/apps and the Matchmaking Agent
Manages Assets (=Internal Addresses including security and other contextual information into the Binding/Virtualization/Backend Layers together with External meta-data/properties of the Assets for discovery and self-description)
Manages Policies (=Conditions regarding the validity of Asset negotiations and interactions)
Manages Contract Definitions (=Offers are combinations of Assets and Policies and are used to build up a Catalogue)
a public (DAPS-secured) IDS API for coordination with other control planes of other business partners to setup transfer routings between the data planes.
state machines for monitoring (data) transfer processes which are actually executed by the (multiple, scalable) data plane(s). KA introduces a new transfer process type “HttpProtocol” which works like the standard “HttpProxy” transfer but enables sub-protocols of http/s (such as SPARQL) installing dedicated routing logic in the data plane(s) (see below)
a validation engine which currently operates on static tokens/claims which are extracted from the transfer flow but may be extended with additional properties in order to check additional runtime information in the form of properties
callback triggers for announcing transfer endpoints to the data plane to external applications, such as the Matchmaking Agent (or other direct EDC clients, frameworks and applications). We want to support multiple Matchmaking Agent instances per EDC for load-balancing purposes and we also like to allow for a bridged operation with other non-KA use cases, so it should be possible to configure several endpoint callback listeners per control plane.
The Data Plane (multiple instances) performs the actual data transfer tasks as instrumented by the control plane. The data plane exposes transfer-specific capabilities (Sinks and Sources) to adapt the actual endpoint/asset protocols (in the EDC standard: the asset type).

Graph Assets use the asset type “urn:cx:Protocol:w3c:Http#SPARQL”. In their address part, the following properties are supported
“asset:prop:id” – The name under which the Graph will be offered. Should be a proper IRI/URN, such as urn:io.catenax.knowledge.dataspace:GraphAsset#TelematicsSupplier
“baseUrl” – The endpoint URL of the binding agent (see below). Should be a proper http/s SPARQL endpoint.
“proxyPath” – must be set to “false”
“proxyQueryParams” – must be set to “true”
“proxyBody” – must be set to “true”
“authKey” – optional authentication header, e.g. “X-Api-Key”
“authCode” – optional authentication value, such as an API key
“header:Host” – optional fixed Host header forwarded to the endpoint
“header:Accepts” – optional fixed Accepts header forwarded to the endpoint, e.g., “application/sparql-results+json”
Skill Assets use the asset type “urn:cx:Protocol:w3c:Http#SKILL”. In their address part, the following properties are supported
“asset:prop:id” – The name under which the Skill will be offered. Should be a proper IRI/URN, such as urn:io.catenax.knowledge.dataspace:SkillAsset#TelematicsOEM
“baseUrl” – should be empty or will be ignored
“query” – A valid and parameterized SPARQL query
“proxyPath” – must be set to “false”
“proxyQueryParams” – must be set to “true”
“proxyBody” – must be set to “false”
In their description part, Skill Assets and Graph Assets can have the following properties:
“asset:prop:id” – Equal to the corresponding field in the internal address.
“cx:hasRole” – An RDF description listing the Use case Roles that this asset belongs to, e.g. “<urn:io.catenax.knowledge.dataspace:UseCaseRole#TelematicsOEM>”
“asset:prop:name” – Title of the asset in the default language.
“asset:prop:name@de” – Title of the asset in German (or other languages accordingly.
“asset:prop:description”, “asset:prop:name@de”, … - Description of the Asset
“asset:prop:version” – A version IRI which is a download URI for the Asset Desccription as a separate file
“asset:prop:contenttype” – A valid Content-Type constraint to list the available response formats, e.g. “application/sparql-results+json, application/sparql-results+xml”
“rdf:type” – Iri of the Asset Type, “<urn:io.catenax.knowledge.dataspace:GraphAsset>” for graph assets and “<urn:io.catenax.knowledge.dataspace:SkillAsset>” for skill assets.
“rdfs:isDefinedBy” – An RDF description listing the Use case ontologies that this asset belongs to, e.g., “<urn:io.catenax.knowledge.dataspace:UseCase#Telematics>”
“cx:protocol” – should be set to “<urn:cx:Protocol:w3c:Http#SPARQL>”
“cx:shape” – contains a SHACL constraint description which describes the contents of the Graph Asset or the SPARQL text of the query (in case this is a “free” Skill)
“cx:isFederated” – a Boolean indicating whether this asset should appear in the federated data catalogue (and hence can be dynamically resolved)
For both Graph and Skill Assets, appropriate Sink and Source implementations have to be registered which operate just as the standard HttpSink and HttpSource, but cater for some additional peculiarities. In particular, the “AgentSource”
should unwrap the original “Accepts” header from the payload that is enclosed within the “cx_accepts” parameter
complete the payload with any additional headers (and the query parameter) as given in the assets address properties.
May parse the query and validate the given data address using additional runtime information from the query, the header, the parameters and extended  policies with the help of the extended control plane.
rewrite the resulting SPARQL query parameter/body by replacing any occurrence of the Asset-URI “GRAPH <?assetUri>” with the actual URL of the asset baseUrl (SERVICE <?baseUrl>).
May rewrite the query using the “cx:shape” property of the GraphAsset in order to enforce particular constraints.
Delegate the resulting call to the Matchmaking Agent.

### Matchmaking Agent

This component which is the first stage of SPARQL processing serves several purposes. It operates as the main invocation point to the Data Consuming Layer. It operates as the main bridging point between incoming EDC transfers (from an “Agent Source”) and the underlying Binding Layer. And it implements federation by delegating any outgoing SERVICE/GRAPH contexts to the EDC. The Matchmaking Agent

Should  perform a realm-mapping from the tenant domain (Authentication Scheme, such as API-Key and Oauth2) into the dataspace domain (EDC tokens)
Should use the EDC management API in order to negotiate outgoing “HttpProtocol” transfers. It may use parallelism and asynchronity to perform multiple such calls simultaneously. It will wrap any inbound “Accept” header requirements as an additional “cx_accept” parameter to the transfer sink.
Should operate as a endpoint callback listener, such that the setup transfers can invoke the data plane
Uses and Maintains the Federated Catalogue as an RDF store.
Should be able to access Binding Agents by means of “SERVICE” contexts in the SPARQL standard. Hereby, the Matchmaking Agent should be able to restrict the type of sub-queries that are forwarded. For practicability purposes, Binding Agents need only support a subset of SPARQL and OWL (no embedded GRAPH/SERVICE contexts, no transitive closures and inversion, no object variables in rdf:type, no owl:sameAs lookups, …).
Since EDC and Matchmaking Agent are bidirectionally coupled, implementations could merge Data Plane and Matchmaking Agent into a single package, the so-called Agent Plane. Agent Planes and ordinary Data Planes can co-exist due to our design choices.

### Federated Catalogue

The Federated Catalogue is an RDF data storage facility for the Matchmaking Agent. It could be an in-memory triple store (that is restored via downloading TTL and configuration files upon restart) or an ordinary relational database that has been adapted to fit to the chosen Matchmaking Agent implementation. One example of such an interface is the RDF4J SAIL compliant to all RDF4J based SPARQL engines.

The Federated Catalogue should initially download the complete Semantic Model that has been released for the target environment. It should also contain a list of business partners and their roles which form the surrounding dataspace neighborhood of the tenant. For that purpose, It could use GPDM and Self-Description Hub services in order to lookup EDC addresses and additional domain information (sites, geo addresses). It should then be frequently updated with “live” information by invoking the EDC data management API to regularly obtain catalogue information.

The portion of the Semantic Model describing these meta-data (Business Partners, Sites, Addresses, Use Cases, Use Case Roles, Connectors & Assets) is called the Common domain ontology and is mandatory for all releases/excerpts of the Semantic Model (<https://raw.githubusercontent.com/catenax-ng/product-knowledge/main/ontology/common_ontology.ttl>).

## Backend Systems (Non-Standard Relevant)

(Legacy, Non-Dataspace) IT landscape of data space participants consisting of various backend systems, such as PLM, ERP, ObjectStores mostly located in the Enterprise Intranet and hosted/goverened by the business departments.
Here, the actual data sources of all Catena-X participants is originated
where they are served using custom, but mission-critical business or
technological APIs in specific, transaction-oriented formats.

### AAS Servers and Databases

As a special case of backend systems, we also regard existing AAS servers and databases as valid data sources
to form a semantic dataspace.

See [AAS Bridge](aas/bridge.md) for a more detailed explanation.

## Virtualization Layer (Non-Standard Relevant)

The data virtualization layer fulfills the function of making the company
internal, department-hosted data available for cross-company data exchange
scenarios, e.g. via data lakes, data warehouses or other enterprise
middleware.

Instead of connecting each and every backend system separately to
an published data source/sink (such as provided by Catena-X) it often makes
sense to have this additional layer on top of backend systems
which orchestrates data demand and supply across the systems.

Depending on company IT architecture different technologies can be used
to build up this layer.

### ETL/Data Lakes

In this scenario data from connected backend systems is stored in a central repository, such as in a Data Lake or central Data Warehouse scenario. Here, different kinds of raw data can be stored, processed, and combined to new data sets, while still being centrally available for any access. Synchronization between backends and data lake is achieved via ETL processes.

### API Gateway

This approach offers users a unified and technically abstract view for querying and manipulating data across a range of disparate sources. As such, it can be used to create virtualized and integrated views of data in memory rather than executing data movement and physically storing integrated views.

## Binding Layer

Finally, the missing link between the Dataspace Layer and the Virtualization Layer is the Binding Layer. Hereby rather than mapping the data between different formats (e.g. Data Tables in CSV Format to and from Data Graphs in the TTL format) which is a mostly resource-consuming data transformation process, binding rather rewrites the actual queries (e.g. SPARQL into SQL, SPARQL into GraphQL or REST). In order to make this query rewriting not too complicated, a restricted subset of SPARQL is envisaged.

### Virtual Knowledge Graph

A Virtual Knowledge Graph has the aim to make the content of relational databases accessable as a virtual knowledge graph. Virtual in this context means that the data itself remains in the relational database. Furthermore, this building block provides the function to translate SPARQL to SQL queries (e.g. via R2RML mappings in TTL).

### Functional Remoting

The Functional Remoting building block allows translation of SPARQL queries to specific API calls, e.g. to trigger a certain computation function. Function Binding is described in a special RDF4J TTL configuration.

### Graph Database

A graph database stores a pre-mapped Knowledge Graph in a dedicated RDF store. It can be combined with a Virtual Knowledge Graph in order to cache frequent accesses to the Virtualization Layer.

### AAS->KA Bridge

Special form of virtualization component which denormalizes/flattens & caches the often hierarchical
information (Shells, Submodels, Submodel Elements) stored in backend AAS servers in order to make it
accessible for ad-hoc querying.

See [AAS Bridge](aas/bridge.md) for a more detailed explanation.

### KA->AAS Bridge

In order to form a twin-based, highly-standarized access to any graphTo allow for a more strict
In order to form a graph-based, flexible access to AAS backend components, we
employ a bridge virtualization module which denormalizes/caches the information
inside Shells and Submodels.

See [AAS Bridge](aas/bridge.md) for a more detailed explanation.
